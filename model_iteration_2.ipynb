{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bumho/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(d, median_age):\n",
    "    \"\"\"\n",
    "    cleans the data with \n",
    "    \"\"\"\n",
    "    \n",
    "    # recodes male as 0 and female as 1\n",
    "    d.loc[d.Sex == 'male', 'Sex'] = 0\n",
    "    d.loc[d.Sex == 'female', 'Sex'] = 1\n",
    "    \n",
    "    # missing age with median\n",
    "    d.Age = d[\"Age\"].fillna(median_age)\n",
    "\n",
    "    # recodes the ports to 0 1 2\n",
    "    d.Embarked = d.Embarked.fillna('S')\n",
    "    d.loc[d.Embarked == 'S', 'Embarked'] = 0\n",
    "    d.loc[d.Embarked == 'C', 'Embarked'] = 1\n",
    "    d.loc[d.Embarked == 'Q', 'Embarked'] = 2\n",
    "    \n",
    "    d[\"Fare\"] = d[\"Fare\"].fillna(d[\"Fare\"].median())\n",
    "    \n",
    "    return d\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train = clean(train, train.Age.median())\n",
    "test = clean(test, train.Age.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.801346801347\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "scores = cross_validation.cross_val_score(alg, train[predictors], train[\"Survived\"], cv=3)\n",
    "print \"accuracy\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest model is already doing better than previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(alg, data, predictors):\n",
    "    return cross_validation.cross_val_score(alg, train[predictors], train[\"Survived\"], cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.820426487093\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "print \"accuracy \", cross_validate(alg, train, predictors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a Random Forest model with more estimaters, samples split and leaf to reduce over fitting and it does much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(file_name, alg, train, test, predictors):\n",
    "    \"\"\"\n",
    "    generates submission csv with alg, train, and predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    alg.fit(train[predictors], train[\"Survived\"])\n",
    "    predictions = alg.predict(test[predictors])\n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': test[\"PassengerId\"],\n",
    "        'Survived': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit(\"kaggle_rf.csv\", alg, train, test, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".76077 for the submission. It did better than my previous attempts. One possiblity why random forest model is not performing super well on the kaggle set is because it might be overfitting. I want to try with samples split and leaf to reducing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.818181818182\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf=5)\n",
    "print \"accuracy\", cross_validate(alg, train, predictors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit(\"kaggle_rf2.csv\", alg, train, test, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this didnt change much ai got the same accuracy as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating functions for all of the transformations since we have to apply them to both train and test\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "        \n",
    "        # Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "        title_mapping = {\n",
    "            \"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "        try:\n",
    "            return title_mapping[title]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def new_features(data):\n",
    "    \n",
    "    # family number column\n",
    "    data[\"FamilyNum\"] = data[\"SibSp\"] + data[\"Parch\"] + 1 # add one to include the individual\n",
    "    \n",
    "    # The .apply method generates a new series\n",
    "    data[\"NameLength\"] = data[\"Name\"].apply(lambda x: len(x))\n",
    "    \n",
    "    # add title\n",
    "    data[\"Title\"] = data.Name.apply(get_title)\n",
    "    \n",
    "    data[\"Child\"] = data[\"Age\"]\n",
    "    data.loc[data[\"Child\"] <= 8, \"Child\"] = 1\n",
    "    data.loc[data[\"Child\"] >8, \"Child\"] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensemble(algorithms, data):\n",
    "    # Initialize the cross validation folds\n",
    "    kf = KFold(data.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "    predictions = []\n",
    "    for train, test in kf:\n",
    "        train_target = data[\"Survived\"].iloc[train]\n",
    "        full_test_predictions = []\n",
    "        # Make predictions for each algorithm on each fold\n",
    "        for alg, predictors in algorithms:\n",
    "            # Fit the algorithm on the training data.\n",
    "            alg.fit(data[predictors].iloc[train,:], train_target)\n",
    "            # Select and predict on the test fold.  \n",
    "            # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "            test_predictions = alg.predict_proba(data[predictors].iloc[test,:].astype(float))[:,1]\n",
    "            full_test_predictions.append(test_predictions)\n",
    "        # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "        test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "        # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "        test_predictions[test_predictions <= .5] = 0\n",
    "        test_predictions[test_predictions > .5] = 1\n",
    "        predictions.append(test_predictions)\n",
    "\n",
    "    # Put all the predictions together into one array.\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    # Compute accuracy by comparing to the training data.\n",
    "    accuracy = sum(predictions[predictions == data[\"Survived\"]]) / len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82379349046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bumho/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algs = [\n",
    "        [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyNum\", \"Child\", \"NameLength\", \"Title\" ]],\n",
    "        [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyNum\", \"Child\", \"NameLength\",\"Title\" ]],\n",
    "        [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf=5), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyNum\", \"Child\", \"NameLength\",\"Title\" ]]\n",
    "    ]\n",
    "new_features(train)\n",
    "new_features(test)\n",
    "print ensemble(algs, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to try combining GradientBoosting, Logistic Regrssion and random forest to see if this makes my result better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submitMultipe(file_name, algs, train, test):\n",
    "    \"\"\"\n",
    "    generates submission csv with alg, train, and predictors\n",
    "    \"\"\"\n",
    "    full_predictions = []\n",
    "    for alg, predictors in algs:\n",
    "        alg.fit(train[predictors], train[\"Survived\"])\n",
    "        \n",
    "        predictions = alg.predict_proba(test[predictors].astype(float))[:,1]\n",
    "        full_predictions.append(predictions)\n",
    "\n",
    "    test_predictions = np.sum(full_predictions, axis=0) / len(full_predictions)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "        \n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': test[\"PassengerId\"],\n",
    "        'Survived': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submitMultipe(\"ensemble.csv\", algs, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did much better with .78469. Combining difference algorithms definitely seemed to have helped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass 24.5956714208\n",
      "Sex 68.8519942529\n",
      "Age 1.27768954597\n",
      "Fare 14.2132351418\n",
      "Embarked 2.85130099045\n",
      "FamilyNum 0.207684583419\n",
      "Child 5.02069887713\n",
      "NameLength 23.6931901615\n",
      "Title 26.9833860721\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilyNum\", \"Child\", \"NameLength\",\"Title\" ]\n",
    "\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(train[predictors], train[\"Survived\"])\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "for i in range(len(scores)):\n",
    "    print predictors[i], scores[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that top 5 categories are Sex, Title, PClass, NameLength, Fare. Let's try running our tests with just these 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bumho/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algs = [\n",
    "        [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\"]],\n",
    "        [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\"]],\n",
    "        [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf=5),[\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\"]]\n",
    "    ]\n",
    "print ensemble(algs, train)\n",
    "submitMultipe(\"5cat.csv\", algs, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores mean was less and it didn't do better on kaggle. I want to try excluding all of the predictors less than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813692480359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bumho/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "algs = [\n",
    "        [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\", \"Fare\", \"Child\", \"Embarked\"]],\n",
    "        [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\", \"Fare\", \"Child\", \"Embarked\"]],\n",
    "        [RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf=5),[\"Pclass\", \"Sex\", \"Title\", \"Fare\", \"NameLength\", \"Fare\", \"Child\", \"Embarked\"]]\n",
    "    ]\n",
    "print ensemble(algs, train)\n",
    "submitMultipe(\"8cat.csv\", algs, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no improvement so it does seem like adding more predictors help our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploration has been very interesting. The data most of the times doesnt seem to behave the way I expected it to be, but I guess that is the beauty of data science. If everything behaved the way we expected to behave then nothing would be really interesting. With this warmup project I think I definitely learned some tools I can use to start my own machine learning journey. \n",
    "I think for my future iterations I want to possibly read more about the background story of titanic to understand the data set better and learn more algorithms to try them out and form a better ensemble of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
